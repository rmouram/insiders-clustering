import re
import numpy as np
import pandas as pd
import seaborn as sns

from matplotlib import pyplot as plt

from sklearn import cluster as c
from sklearn import metrics as m

from plotly import express as px

from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer

import umap.umap_ as umap


# laod dataset
df_raw = pd.read_csv('../data/Ecommerce.csv', encoding = 'unicode_escape')

# drop extra column
df_raw = df_raw.drop(columns = ['Unnamed: 8'], axis=1)


df_raw.columns


df1 = df_raw.copy()


cols_new = ['invoice_no', 'stock_code', 'description', 'quantity', 'invoice_date', 'unit_price', 'customer_id', 'country']

df1.columns  = cols_new


print('Number of rows: {}'.format(df1.shape[0]))
print('Number of cols: {}'.format(df1.shape[1]))


df1.dtypes


df1.head()


df1.isna().sum()


# remove NA
df1 = df1.dropna(subset=['description', 'customer_id'])
print('Removed data: {:.2f}'.format( 1-( df1.shape[0] / df_raw.shape[0] ) ) )


df1.isna().sum()


# invoice date
df1['invoice_date'] = pd.to_datetime(df1['invoice_date'], format = '%d-%b-%y')


df1.dtypes


num_attributes = df1.select_dtypes(include=['float64', 'int64'])
cat_attributes = df1.select_dtypes(exclude=['float64', 'int64', 'datetime64[ns]'])


# central tendency - mean, median
c1 = pd.DataFrame(num_attributes.apply(np.mean)).T
c2 = pd.DataFrame(num_attributes.apply(np.median)).T

# dispersion - min, max, range, standard deviation, skew, kurtosis
d1 = pd.DataFrame(num_attributes.apply(np.min)).T
d2 = pd.DataFrame(num_attributes.apply(np.max)).T
d3 = pd.DataFrame(num_attributes.apply(lambda x: x.max() - x.mean())).T
d4 = pd.DataFrame(num_attributes.apply(np.std)).T
d5 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T
d6 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T

m = pd.concat([c1, c2, d4, d1, d2, d3, d5, d6]).T.reset_index()
m.columns = ['attributes', 'mean', 'median', 'std', 'min','max','range','skew','kurtosis']
m


cat_attributes.head()


# invoice number with letter
len(cat_attributes.loc[cat_attributes['invoice_no'].apply(lambda x: bool(re.search('[^0-9]+', x))), 'invoice_no'].drop_duplicates())


# stock code with letter
len(cat_attributes.loc[cat_attributes['stock_code'].apply(lambda x: bool(re.search('[^0-9]+', x))), 'stock_code'].drop_duplicates())


# stock code only letter
cat_attributes.loc[cat_attributes['stock_code'].apply(lambda x: bool(re.search('^[a-zA-Z]+$', x))), 'stock_code'].drop_duplicates()


df2 = df1.copy()


df2.loc[df2['quantity'] < 0, :].sort_values(['customer_id', 'description']).head(10)


# ======= Numerical Attributes =======
# unit price
df2 = df2.loc[df2['unit_price'] > 0.010, :]

# stock code != [POST, M, D, DOT, CRUK]
df2 = df2[~df2['stock_code'].isin(['POST','D','M','DOT','CRUK'])]

# quantity
df2_returns = df2.loc[df2['quantity'] < 0, :]
df2_purchase = df2.loc[df2['quantity'] >  0, :]


df2_returns.head()


df2_purchase.head()


df3 = df2_purchase.copy()


# data reference
df_ref = df3.drop(['invoice_no', 'stock_code', 'description', 'quantity', 'invoice_date', 'unit_price', 'country'], axis = 1).drop_duplicates(ignore_index=True)
df_ref.head()


# Gross Revenue (Faturamento) quantity * price
df3['gross_revenue'] = df3['quantity'] * df3['unit_price']


# Monetary
df_monetary = df3[['customer_id', 'gross_revenue']].groupby('customer_id').sum().reset_index()
df_ref = pd.merge(df_ref, df_monetary, on='customer_id', how='left')
df_ref.isna().sum()


# Recency - Last day purchase
df_recency = df3[['customer_id', 'invoice_date']].groupby('customer_id').max().reset_index()
df_recency['recency_days'] = (df2['invoice_date'].max() - df_recency['invoice_date']).dt.days
df_recency = df_recency[['customer_id', 'recency_days']].copy()
df_ref = pd.merge(df_ref, df_recency, on='customer_id', how='left')
df_ref.isna().sum()


# Frequency - Quantity 
df_freq = df3[['customer_id', 'invoice_no']].drop_duplicates().groupby('customer_id').count().reset_index()
df_ref = pd.merge(df_ref, df_freq, on='customer_id', how='left')
df_ref.isna().sum()


# Avg Ticket
df_avg_ticket = df3[['customer_id', 'gross_revenue']].groupby('customer_id').mean().reset_index().rename(columns={'gross_revenue':'avg_ticket'})
df_ref = pd.merge(df_ref, df_avg_ticket, on='customer_id', how='left')
df_ref.isna().sum()


# Number of returns
df_returns = df2_returns[['customer_id','invoice_no']].drop_duplicates().groupby('customer_id').count().reset_index().rename(columns={'invoice_no':'returns'})
df_ref = pd.merge(df_ref, df_returns, on='customer_id', how='left')
df_ref.loc[df_ref['returns'].isna(), 'returns'] = 0
df_ref.isna().sum()


df_ref


df4 = df_ref.copy()


df4.head()


print('Quantidade de clientes: {}'.format(df4['customer_id'].count()))


df5 = df4.copy()


df6 = df5.copy()


X = df6.drop(columns=['customer_id'])


clusters = [2, 3, 4, 5, 6, 7]


wss = []
for k in clusters:
    # model definition
    kmeans = c.KMeans(init='random', n_clusters=k, n_init=10, max_iter=300, random_state=42)

    #model training
    kmeans.fit(X)

    # validation
    wss.append(kmeans.inertia_)

# plot wss
plt.plot(clusters, wss, linestyle='--', marker='o', color='b')
plt.xlabel('K')
plt.ylabel('Within-Cluster Sum of Square')
plt.title('WSS vs K')


kmeans = KElbowVisualizer(c.KMeans(), k=clusters, timings = False)
kmeans.fit(X)
kmeans.show()


kmeans = KElbowVisualizer(c.KMeans(), k=clusters, metric='silhouette', timings = False)
kmeans.fit(X)
kmeans.show()


k = [2, 3, 4, 5, 6, 7]

fig, ax = plt.subplots(3, 2, figsize=(25, 18))

for k in clusters:
    km = c.KMeans(init='random', n_clusters=k, n_init=10, max_iter=300, random_state=42)
    q, mod = divmod(k, 2)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(X)
    visualizer.finalize()


# model definition
k = 6
kmeans = c.KMeans(init='random', n_clusters=k, n_init=10, max_iter=300, random_state=42)

# model training
kmeans.fit(X)

# clustering
labels = kmeans.labels_


# WSS (Within Cluster sum of square)
print('WSS: {}'.format(kmeans.inertia_))

# SS (Silhouette Score)
print('SS: {}'.format(m.silhouette_score(X, labels, metric='euclidean')))


df9 = df6.copy()
df9['cluster'] = labels


df9.head()


k = 4
km = c.KMeans(init='random', n_clusters=k, n_init=10, max_iter=300, random_state=42)
visualizer = SilhouetteVisualizer(km, colors='yellowbrick')
visualizer.fit(X)
visualizer.finalize()


df_viz = df9.drop(columns='customer_id', axis=1)
sns.pairplot(df_viz, hue='cluster')


reducer = umap.UMAP(n_neighbors=30, random_state=42)
embedding = reducer.fit_transform(X)

#embedding
df_viz['embedding_x'] = embedding[:, 0]
df_viz['embedding_y'] = embedding[:, 1]

#plot UMAP
sns.scatterplot(x='embedding_x', y='embedding_y', hue='cluster', palette=sns.color_palette('hls', n_colors=len(df_viz['cluster'].unique())), data=df_viz)


# number of customers
df_cluster = df9[['customer_id', 'cluster']].groupby('cluster').count().reset_index()
df_cluster['perc_customers'] = 100 * (df_cluster['customer_id']/df_cluster['customer_id'].sum())

# gross revenue
df_gross_revenue = df9[['gross_revenue', 'cluster']].groupby('cluster').mean().reset_index()
df_cluster = pd.merge(df_cluster, df_gross_revenue, how='inner', on='cluster')

# recency days
df_recency_days = df9[['recency_days', 'cluster']].groupby('cluster').mean().reset_index()
df_cluster = pd.merge(df_cluster, df_recency_days, how='inner', on='cluster')

# invoice number
df_invoice_no = df9[['invoice_no', 'cluster']].groupby('cluster').mean().reset_index()
df_cluster = pd.merge(df_cluster, df_invoice_no, how='inner', on='cluster')

# Average Ticket
df_avg_ticket = df9[['avg_ticket', 'cluster']].groupby('cluster').mean().reset_index()
df_cluster = pd.merge(df_cluster, df_avg_ticket, how='inner', on='cluster')

# Number of returns
df_returns = df9[['returns', 'cluster']].groupby('cluster').mean().reset_index()
df_cluster = pd.merge(df_cluster, df_returns, how='inner', on='cluster')

df_cluster.sort_values('perc_customers')
